{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clothes classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33650\\.virtualenvs\\ai_model-PGgiT5Dx\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category probabilities:\n",
      "Robe: 0.0001\n",
      "T-shirt: 0.9947\n",
      "Pantalon: 0.0004\n",
      "Veste: 0.0002\n",
      "Sous-vêtements: 0.0005\n",
      "Chaussures: 0.0000\n",
      "Chapeau: 0.0000\n",
      "Pull: 0.0041\n",
      "\n",
      "The most probable category is: T-shirt\n"
     ]
    }
   ],
   "source": [
    "# Load your image and possible category descriptions\n",
    "image = Image.open(\"img_test/T-shirt.jpg\")\n",
    "categories = [\"a photo of a dress\", \"a photo of a t-shirt\", \"a photo of pants\", \"a photo of a jacket\", \"a photo of underwear\", \"a photo of shoes\", \"a photo of hat\", \"a photo of a pullover\"]\n",
    "\n",
    "# Preprocess the image and text for CLIP\n",
    "inputs = processor(text=categories, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Perform classification\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # Convert to probabilities\n",
    "\n",
    "# Convert the tensor to a list\n",
    "probs_list = probs.squeeze().tolist()  # Remove batch dimension and convert to list\n",
    "\n",
    "categories_fr = [\"Robe\", \"T-shirt\", \"Pantalon\", \"Veste\", \"Sous-vêtements\", \"Chaussures\", \"Chapeau\", \"Pull\"]\n",
    "\n",
    "# Create a dictionary to map categories to their corresponding probabilities\n",
    "category_prob_dict = dict(zip(categories_fr, probs_list))\n",
    "\n",
    "# Print the dictionary\n",
    "print(\"Category probabilities:\")\n",
    "for category, prob in category_prob_dict.items():\n",
    "    print(f\"{category}: {prob:.4f}\")\n",
    "\n",
    "# Find the category with the highest probability\n",
    "most_probable_category = max(category_prob_dict, key=category_prob_dict.get)\n",
    "print(f\"\\nThe most probable category is: {most_probable_category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX format for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33650\\.virtualenvs\\ai_model-PGgiT5Dx\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "c:\\Users\\33650\\.virtualenvs\\ai_model-PGgiT5Dx\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "c:\\Users\\33650\\.virtualenvs\\ai_model-PGgiT5Dx\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:5385: UserWarning: Exporting aten::index operator of advanced indexing in opset 20 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, (inputs['input_ids'], inputs['pixel_values']), \"clip_clothes_classification.onnx\", \n",
    "                  input_names=[\"input_ids\", \"pixel_values\"],\n",
    "                  output_names=[\"logits_per_image\", \"logits_per_text\"],\n",
    "                  dynamic_axes={'input_ids': {0: 'batch_size'}, 'pixel_values': {0: 'batch_size'}}, opset_version=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize the model to reduce its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic quantization completed. Quantized model saved at clip_clothes_classification_quant.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Apply dynamic quantization\n",
    "quantized_model_path = \"clip_clothes_classification_quant.onnx\"\n",
    "quantize_dynamic(\n",
    "    \"clip_clothes_classification.onnx\",\n",
    "    quantized_model_path,\n",
    "    weight_type=QuantType.QUInt8\n",
    ")\n",
    "print(f\"Dynamic quantization completed. Quantized model saved at {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the ONNX model with ONNXRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category probabilities:\n",
      "Robe: 0.0047\n",
      "T-shirt: 0.9627\n",
      "Pantalon: 0.0053\n",
      "Veste: 0.0057\n",
      "Sous-vêtements: 0.0084\n",
      "Chaussures: 0.0005\n",
      "Chapeau: 0.0064\n",
      "Pull: 0.0063\n",
      "\n",
      "The most probable category is: T-shirt\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "ort_session = ort.InferenceSession(\"clip_clothes_classification_quant.onnx\")\n",
    "\n",
    "input_ids = inputs['input_ids'].numpy()\n",
    "pixel_values = inputs['pixel_values'].numpy()\n",
    "\n",
    "# Run the inference\n",
    "outputs = ort_session.run(None, {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"pixel_values\": pixel_values,\n",
    "})\n",
    "\n",
    "logits = torch.Tensor(outputs[0]).softmax(dim=1)\n",
    "\n",
    "# Convert the tensor to a list\n",
    "probs_list = logits.squeeze().tolist()  # Remove batch dimension and convert to list\n",
    "\n",
    "categories_fr = [\"Robe\", \"T-shirt\", \"Pantalon\", \"Veste\", \"Sous-vêtements\", \"Chaussures\", \"Chapeau\", \"Pull\"]\n",
    "\n",
    "# Create a dictionary to map categories to their corresponding probabilities\n",
    "category_prob_dict = dict(zip(categories_fr, probs_list))\n",
    "\n",
    "# Print the dictionary\n",
    "print(\"Category probabilities:\")\n",
    "for category, prob in category_prob_dict.items():\n",
    "    print(f\"{category}: {prob:.4f}\")\n",
    "\n",
    "# Find the category with the highest probability\n",
    "most_probable_category = max(category_prob_dict, key=category_prob_dict.get)\n",
    "print(f\"\\nThe most probable category is: {most_probable_category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the input_ids (tokenized categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to export the tokenized categories so we can use it in the flutter app as input for the onnx model since the model does not include the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dart list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dart array saved to input_ids.dart\n"
     ]
    }
   ],
   "source": [
    "# Format the list as a Dart array string\n",
    "dart_array_str = f\"const List<List<int>> inputIds = {input_ids};\"\n",
    "\n",
    "# Save it to a Dart file (optional)\n",
    "with open(\"input_ids.dart\", \"w\") as dart_file:\n",
    "    dart_file.write(dart_array_str)\n",
    "\n",
    "print(\"Dart array saved to input_ids.dart\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_model-PGgiT5Dx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
