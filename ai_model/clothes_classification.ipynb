{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clothes classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apple MobileCLIP model\n",
    "\n",
    "https://huggingface.co/apple/MobileCLIP-S1-OpenCLIP\n",
    "\n",
    "Similar to OpenAI CLIP model but lighter and more performant (therefore more suitable for mobile deployment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python inference test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip  # OpenCLIP library\n",
    "\n",
    "# Load the MobileCLIP-S1 model and tokenizer\n",
    "model, _, processor = open_clip.create_model_and_transforms('MobileCLIP-S1', pretrained='datacompdr')\n",
    "model.eval()\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer('MobileCLIP-S1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category probabilities:\n",
      "Robe: 0.0000\n",
      "T-shirt: 0.0000\n",
      "Pantalon: 0.0000\n",
      "Veste: 0.0000\n",
      "Sous-vêtements: 0.0000\n",
      "Chaussures: 0.0000\n",
      "Chapeau: 0.0000\n",
      "Pull: 1.0000\n",
      "\n",
      "The most probable category is: Pull\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image and text categories\n",
    "image = Image.open(\"img_test/pull.jpg\")\n",
    "categories = [\n",
    "    \"a photo of a dress\", \"a photo of a t-shirt\", \"a photo of pants\",\n",
    "    \"a photo of a jacket\", \"a photo of underwear\", \"a photo of shoes\",\n",
    "    \"a photo of hat\", \"a photo of a pullover\"\n",
    "]\n",
    "\n",
    "# Tokenize text inputs (using OpenCLIP tokenizer)\n",
    "\n",
    "text_inputs = tokenizer(categories)\n",
    "\n",
    "# Preprocess the image using OpenCLIP processor\n",
    "image_input = processor(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Move model and inputs to the appropriate device (e.g., GPU if available)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "image_input = image_input.to(device)\n",
    "text_inputs = text_inputs.to(device)\n",
    "\n",
    "# Perform classification\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    logits_per_image = (100 * image_features @ text_features.T).softmax(dim=-1)  # Image-text similarity score\n",
    "    probs = logits_per_image.cpu().squeeze().tolist()  # Convert to probabilities\n",
    "\n",
    "# Map categories to probabilities (e.g., for French categories)\n",
    "categories_fr = [\"Robe\", \"T-shirt\", \"Pantalon\", \"Veste\", \"Sous-vêtements\", \"Chaussures\", \"Chapeau\", \"Pull\"]\n",
    "category_prob_dict = dict(zip(categories_fr, probs))\n",
    "\n",
    "# Print category probabilities\n",
    "print(\"Category probabilities:\")\n",
    "for category, prob in category_prob_dict.items():\n",
    "    print(f\"{category}: {prob:.4f}\")\n",
    "\n",
    "# Find the category with the highest probability\n",
    "most_probable_category = max(category_prob_dict, key=category_prob_dict.get)\n",
    "print(f\"\\nThe most probable category is: {most_probable_category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX format for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33650\\.virtualenvs\\ai_model-PGgiT5Dx\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:5385: UserWarning: Exporting aten::index operator of advanced indexing in opset 20 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    (image_input, text_inputs.to(torch.int32)),\n",
    "    \"mobileclip_s1.onnx\",\n",
    "    input_names=[\"pixel_values\", \"input_ids\"],\n",
    "    output_names=[\"logits_per_image\", \"logits_per_text\"],\n",
    "    dynamic_axes={'input_ids': {0: 'batch_size'}, 'pixel_values': {0: 'batch_size'}},\n",
    "    opset_version=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize the model to reduce its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic quantization completed. Quantized model saved at mobileclip_s1_quant.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType, quant_pre_process\n",
    "\n",
    "# Apply dynamic quantization\n",
    "quantized_model_path = \"mobileclip_s1_quant.onnx\"\n",
    "quantize_dynamic(\n",
    "    \"mobileclip_s1.onnx\",\n",
    "    quantized_model_path,\n",
    "    weight_type=QuantType.QUInt8,\n",
    ")\n",
    "print(f\"Dynamic quantization completed. Quantized model saved at {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test inference on ONNXRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image and text categories\n",
    "image = Image.open(\"img_test/pant.jpg\")\n",
    "categories = [\n",
    "    \"a photo of a dress\", \"a photo of a t-shirt\", \"a photo of pants\",\n",
    "    \"a photo of a jacket\", \"a photo of underwear\", \"a photo of shoes\",\n",
    "    \"a photo of hat\", \"a photo of a pullover\"\n",
    "]\n",
    "\n",
    "# Tokenize text inputs (using OpenCLIP tokenizer)\n",
    "\n",
    "text_inputs = tokenizer(categories)\n",
    "\n",
    "# Preprocess the image using OpenCLIP processor\n",
    "image_input = processor(image).unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category probabilities (ONNXRuntime):\n",
      "Robe: 0.1033\n",
      "T-shirt: 0.0530\n",
      "Pantalon: 0.0461\n",
      "Veste: 0.0681\n",
      "Sous-vêtements: 0.0066\n",
      "Chaussures: 0.0044\n",
      "Chapeau: 0.2401\n",
      "Pull: 0.4784\n",
      "\n",
      "The most probable category is: Pull\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = \"mobileclip_s1_quant.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Prepare inputs for ONNXRuntime (convert to numpy arrays)\n",
    "image_input_np = image_input.cpu().numpy()  # Image input from preprocessing\n",
    "text_inputs_np = text_inputs.cpu().numpy()  # Text input from tokenization\n",
    "\n",
    "# Run inference using ONNXRuntime\n",
    "ort_inputs = {\n",
    "    \"pixel_values\": image_input_np,\n",
    "    \"input_ids\": text_inputs_np.astype(np.int32)\n",
    "}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# Extract the embeddings (image and text features)\n",
    "image_features = ort_outputs[0]  # Shape: (1, 512)\n",
    "text_features = ort_outputs[1]  # Shape: (8, 512)\n",
    "\n",
    "# Convert to torch tensors for further manipulation\n",
    "image_features = torch.tensor(image_features)\n",
    "text_features = torch.tensor(text_features)\n",
    "\n",
    "# Normalize the features (same as in PyTorch model)\n",
    "image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Compute the similarity (dot product) between image and text embeddings\n",
    "similarity = (image_features @ text_features.T) * 100  # Shape: (1, 8)\n",
    "probs = similarity.softmax(dim=-1).squeeze().tolist()  # Apply softmax to get probabilities\n",
    "\n",
    "# Map categories to probabilities (same as PyTorch)\n",
    "categories_fr = [\"Robe\", \"T-shirt\", \"Pantalon\", \"Veste\", \"Sous-vêtements\", \"Chaussures\", \"Chapeau\", \"Pull\"]\n",
    "category_prob_dict = dict(zip(categories_fr, probs))\n",
    "\n",
    "# Print category probabilities\n",
    "print(\"Category probabilities (ONNXRuntime):\")\n",
    "for category, prob in category_prob_dict.items():\n",
    "    print(f\"{category}: {prob:.4f}\")\n",
    "\n",
    "# Find the category with the highest probability\n",
    "most_probable_category = max(category_prob_dict, key=category_prob_dict.get)\n",
    "print(f\"\\nThe most probable category is: {most_probable_category}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI CLIP model\n",
    "\n",
    "https://huggingface.co/openai/clip-vit-base-patch32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33650\\.virtualenvs\\ai_model-PGgiT5Dx\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category probabilities:\n",
      "Robe: 0.0001\n",
      "T-shirt: 0.9947\n",
      "Pantalon: 0.0004\n",
      "Veste: 0.0002\n",
      "Sous-vêtements: 0.0005\n",
      "Chaussures: 0.0000\n",
      "Chapeau: 0.0000\n",
      "Pull: 0.0041\n",
      "\n",
      "The most probable category is: T-shirt\n"
     ]
    }
   ],
   "source": [
    "# Load your image and possible category descriptions\n",
    "image = Image.open(\"img_test/T-shirt.jpg\")\n",
    "categories = [\"a photo of a dress\", \"a photo of a t-shirt\", \"a photo of pants\", \"a photo of a jacket\", \"a photo of underwear\", \"a photo of shoes\", \"a photo of hat\", \"a photo of a pullover\"]\n",
    "\n",
    "# Preprocess the image and text for CLIP\n",
    "inputs = processor(text=categories, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Perform classification\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # Image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # Convert to probabilities\n",
    "\n",
    "# Convert the tensor to a list\n",
    "probs_list = probs.squeeze().tolist()  # Remove batch dimension and convert to list\n",
    "\n",
    "categories_fr = [\"Robe\", \"T-shirt\", \"Pantalon\", \"Veste\", \"Sous-vêtements\", \"Chaussures\", \"Chapeau\", \"Pull\"]\n",
    "\n",
    "# Create a dictionary to map categories to their corresponding probabilities\n",
    "category_prob_dict = dict(zip(categories_fr, probs_list))\n",
    "\n",
    "# Print the dictionary\n",
    "print(\"Category probabilities:\")\n",
    "for category, prob in category_prob_dict.items():\n",
    "    print(f\"{category}: {prob:.4f}\")\n",
    "\n",
    "# Find the category with the highest probability\n",
    "most_probable_category = max(category_prob_dict, key=category_prob_dict.get)\n",
    "print(f\"\\nThe most probable category is: {most_probable_category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX format for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33650\\.virtualenvs\\ai_model-PGgiT5Dx\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "c:\\Users\\33650\\.virtualenvs\\ai_model-PGgiT5Dx\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "c:\\Users\\33650\\.virtualenvs\\ai_model-PGgiT5Dx\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:5385: UserWarning: Exporting aten::index operator of advanced indexing in opset 20 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, (inputs['input_ids'], inputs['pixel_values']), \"clip_clothes_classification.onnx\", \n",
    "                  input_names=[\"input_ids\", \"pixel_values\"],\n",
    "                  output_names=[\"logits_per_image\", \"logits_per_text\"],\n",
    "                  dynamic_axes={'input_ids': {0: 'batch_size'}, 'pixel_values': {0: 'batch_size'}}, opset_version=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize the model to reduce its size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic quantization completed. Quantized model saved at clip_clothes_classification_quant.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Apply dynamic quantization\n",
    "quantized_model_path = \"clip_clothes_classification_quant.onnx\"\n",
    "quantize_dynamic(\n",
    "    \"clip_clothes_classification.onnx\",\n",
    "    quantized_model_path,\n",
    "    weight_type=QuantType.QUInt8\n",
    ")\n",
    "print(f\"Dynamic quantization completed. Quantized model saved at {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the ONNX model with ONNXRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category probabilities:\n",
      "Robe: 0.0047\n",
      "T-shirt: 0.9627\n",
      "Pantalon: 0.0053\n",
      "Veste: 0.0057\n",
      "Sous-vêtements: 0.0084\n",
      "Chaussures: 0.0005\n",
      "Chapeau: 0.0064\n",
      "Pull: 0.0063\n",
      "\n",
      "The most probable category is: T-shirt\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "ort_session = ort.InferenceSession(\"clip_clothes_classification_quant.onnx\")\n",
    "\n",
    "input_ids = inputs['input_ids'].numpy()\n",
    "pixel_values = inputs['pixel_values'].numpy()\n",
    "\n",
    "# Run the inference\n",
    "outputs = ort_session.run(None, {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"pixel_values\": pixel_values,\n",
    "})\n",
    "\n",
    "logits = torch.Tensor(outputs[0]).softmax(dim=1)\n",
    "\n",
    "# Convert the tensor to a list\n",
    "probs_list = logits.squeeze().tolist()  # Remove batch dimension and convert to list\n",
    "\n",
    "categories_fr = [\"Robe\", \"T-shirt\", \"Pantalon\", \"Veste\", \"Sous-vêtements\", \"Chaussures\", \"Chapeau\", \"Pull\"]\n",
    "\n",
    "# Create a dictionary to map categories to their corresponding probabilities\n",
    "category_prob_dict = dict(zip(categories_fr, probs_list))\n",
    "\n",
    "# Print the dictionary\n",
    "print(\"Category probabilities:\")\n",
    "for category, prob in category_prob_dict.items():\n",
    "    print(f\"{category}: {prob:.4f}\")\n",
    "\n",
    "# Find the category with the highest probability\n",
    "most_probable_category = max(category_prob_dict, key=category_prob_dict.get)\n",
    "print(f\"\\nThe most probable category is: {most_probable_category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the input_ids (tokenized categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to export the tokenized categories so we can use it in the flutter app as input for the onnx model since the model does not include the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dart list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dart array saved to input_ids.dart\n"
     ]
    }
   ],
   "source": [
    "# Format the list as a Dart array string\n",
    "dart_array_str = f\"const List<List<int>> inputIds = {input_ids};\"\n",
    "\n",
    "# Save it to a Dart file (optional)\n",
    "with open(\"input_ids.dart\", \"w\") as dart_file:\n",
    "    dart_file.write(dart_array_str)\n",
    "\n",
    "print(\"Dart array saved to input_ids.dart\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_model-PGgiT5Dx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
